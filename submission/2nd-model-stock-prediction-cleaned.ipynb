{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Model : Stock Price Prediction using Graph Convolutional Layer. The model uses GCN and MLP. The input graphs were created using Pearson, Spearman, and Kendal Tau correlations/coefficients. Also, another graph is created based on financial news articles\n",
    "\n",
    "# For the sake of making execution easier (and at once), I have kept multiple approaches in the same file. Because I initially tried separately and brought them together, some code might be a bit redundant/repeating. I may or may not have cleaned enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiTeJ1lXcfkc"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries for Graph, GNN, and GCN\n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learnig related library Imports\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to drop NAN column or row wise\n",
    "drop_cols_with_na = 1\n",
    "drop_rows_with_na = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: Use Fortune 30 companies as the paper used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = pd.DataFrame();\n",
    "data_file = \"per-day-fortune-30-company-stock-price-data.csv\";\n",
    "df_s = pd.read_csv(\"./data/\" + data_file, low_memory = False);\n",
    "df_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cure data such as replace missing/null values, use correct data type, sort by date (not really requured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Date field to be a Date Type\n",
    "df_s[\"Date\"] = df_s[\"Date\"].astype('datetime64[ns]')\n",
    "\n",
    "# Sort data by date although this is no longer needed as data already is sorted\n",
    "#df_s = df_s.sort_values( by = ['Ticker','Date'], ascending = True )\n",
    "df_s = df_s.sort_values( by = 'Date', ascending = True )\n",
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\n",
    "df_s_transpose = df_s\n",
    "\n",
    "try:\n",
    "  df_s_transpose = df_s_transpose.interpolate(inplace = False)\n",
    "except:\n",
    "  print(\"An exception occurred. Operation ignored\")\n",
    "  exit\n",
    "    \n",
    "df_s_transpose.isnull().values.any()\n",
    "df_s_transpose[df_s_transpose.isna().any(axis = 1)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_transpose = df_s\n",
    "\n",
    "if drop_cols_with_na == 1:\n",
    "    df_s_transpose = df_s_transpose.dropna(axis = 1);    \n",
    "   \n",
    "df_s_transpose, df_s_transpose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_transpose.isnull().values.any()\n",
    "df_s_transpose[df_s_transpose.isna().any( axis = 1 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_s_transpose.index = df_s_transpose['Date']\n",
    "df_s_transpose.index = df_s_transpose.index.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXauDrpHbcE3"
   },
   "source": [
    "# Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1678154403155,
     "user": {
      "displayName": "Sayed Ahmed",
      "userId": "06810739430240035128"
     },
     "user_tz": 300
    },
    "id": "BQ3QiHQBV0X1",
    "outputId": "dcfb64df-b60b-46b1-d20e-7961ae8f655f"
   },
   "outputs": [],
   "source": [
    "df_s_transpose_pearson = df_s_transpose.corr(method = 'pearson', numeric_only = True)\n",
    "df_s_transpose_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAF7cHrYbWbJ"
   },
   "source": [
    "# Pearson Correlation Coefficient based Adjacency Graph Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678154403155,
     "user": {
      "displayName": "Sayed Ahmed",
      "userId": "06810739430240035128"
     },
     "user_tz": 300
    },
    "id": "FAKiowgOZkxu",
    "outputId": "1376d8c4-12a0-43ec-f82e-002979c4a7b2"
   },
   "outputs": [],
   "source": [
    "df_s_transpose_pearson[df_s_transpose_pearson >= 0.5] = 1\n",
    "df_s_transpose_pearson[df_s_transpose_pearson < 0.5] = 0\n",
    "df_s_transpose_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the diagonal element to be zero. No self loop\n",
    "import numpy as np\n",
    "np.fill_diagonal(df_s_transpose_pearson.values, 0)\n",
    "df_s_transpose_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d51JMw1Wx21C"
   },
   "source": [
    "Create and visualize the Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UogyyrI1vZnU"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "Graph_pearson = nx.Graph(df_s_transpose_pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 1401,
     "status": "ok",
     "timestamp": 1678154405299,
     "user": {
      "displayName": "Sayed Ahmed",
      "userId": "06810739430240035128"
     },
     "user_tz": 300
    },
    "id": "ejVAFZo2wPdk",
    "outputId": "c4cbe487-95c7-49d1-ed3b-67a38c446264"
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(Graph_pearson, pos=nx.circular_layout(Graph_pearson), node_color='r', edge_color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "altlF1cynQhs"
   },
   "source": [
    "# Create GCN layer. Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all stocks = nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvement: make sure only stocks/nodes that are in the graph are taken\n",
    "all_stock_nodes = df_s_transpose_pearson.index.to_list()\n",
    "all_stock_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all edges between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [];\n",
    "target = [];\n",
    "edge_feature = [];\n",
    "\n",
    "for aStock in all_stock_nodes:\n",
    "    for anotherStock in all_stock_nodes:\n",
    "        if df_s_transpose_pearson[aStock][anotherStock] > 0:\n",
    "            #print(df_s_transpose_pearson[aStock][anotherStock])\n",
    "            source.append(aStock)\n",
    "            target.append(anotherStock)\n",
    "            edge_feature.append(1)\n",
    "            \n",
    "source, target, edge_feature            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables to create stellar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-pandas.html\n",
    "pearson_edges = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target}\n",
    ")\n",
    "\n",
    "pearson_edges_data = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target, \"edge_feature\": edge_feature}\n",
    ")\n",
    "\n",
    "\n",
    "pearson_edges[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Graph with No Feature Data, No node data, only edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pearson_graph = StellarGraph(edges = pearson_edges, node_type_default=\"corner\", edge_type_default=\"line\")\n",
    "#pearson_graph = StellarGraph(nodes = all_stock_nodes, edges = pearson_edges)\n",
    "#graph = sg.StellarGraph(all_stock_nodes, square_edges)\n",
    "print(pearson_graph.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# have the time series data as part of the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_s_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure the Feature Matrix so that it can be passed to the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_transpose_feature = df_s_transpose.reset_index(drop = True, inplace = False)\n",
    "# df_s_transpose_feature =  df_s_transpose_feature.values.tolist()\n",
    "# print(df_s_transpose_feature.values.tolist())\n",
    "#df_s_transpose_feature['WY'].values\n",
    "df_s_transpose_feature['AAPL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_Data = [];\n",
    "for x in all_stock_nodes:\n",
    "    node_Data.append( df_s_transpose_feature[x].values)\n",
    "    \n",
    "    \n",
    "node_Data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_graph_node_data = pd.DataFrame(node_Data, index = all_stock_nodes)\n",
    "pearson_graph_node_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph (stellar) with feature as part of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_graph_with_node_features = StellarGraph(pearson_graph_node_data, edges = pearson_edges, node_type_default = \"corner\", edge_type_default = \"line\")\n",
    "print(pearson_graph_with_node_features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "generator = FullBatchNodeGenerator(pearson_graph_with_node_features, method = \"gcn\") # , sparse = False\n",
    "vars(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    pearson_graph_node_data #, train_size = 6, test_size = 4\n",
    ")\n",
    "# , train_size=6, test_size=None, stratify=pearson_graph_node_data\n",
    "\n",
    "val_subjects, test_subjects_step_2 = model_selection.train_test_split(\n",
    "    test_subjects #, test_size = 2\n",
    ")\n",
    "\n",
    "#, train_size = 500, test_size = None, stratify = test_subjects\n",
    "\n",
    "\n",
    "train_subjects.shape, test_subjects.shape, val_subjects.shape, test_subjects_step_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_graph_node_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train_subjects; \n",
    "val_targets = val_subjects; \n",
    "test_targets = test_subjects; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_subjects.index, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "train_subjects.index, \n",
    "train_targets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train data size\n",
    "#it is not must to use a number like unit_count\n",
    "unit_count = train_subjects.shape[0]\n",
    "unit_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model for all of the approaches utilized in this file\n",
    "# Model for Pearson, Spearman, Kendal Tau, Financial News Based prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D, 2d, 3D CNN: https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610\n",
    "import tensorflow as tf\n",
    "\n",
    "layer_sizes = [32, 32]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "\n",
    "gcn = GCN(layer_sizes = layer_sizes, activations = activations, generator = generator) #, dropout = 0.5\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "input_shape = (1, 21, 753)\n",
    "x_out = Conv1D(filters = 1, kernel_size = 1,  activation='relu', strides=1, input_shape = input_shape)(x_out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#x_out = tf.keras.layers.Flatten()(x_out)\n",
    "\n",
    "# x_out, pool_size=2, strides=1, padding='VALID'\n",
    "#x_out = tf.keras.layers.MaxPooling1D(pool_size=1, strides=1, padding='VALID')(x_out)\n",
    "print(x_out.shape)\n",
    "\n",
    "# [(length_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1]\n",
    "\n",
    "\n",
    "\n",
    "#x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "#x_out = Conv1D(filters = 32, kernel_size = sum(layer_sizes))(x_out)\n",
    "#prediction = keras.layers.Reshape((-1,))(prediction)\n",
    "#x_out = keras.layers.Reshape((1,16))(x_out)\n",
    "#x_out = GCN(layer_sizes = layer_sizes, activations = activations, generator = generator)(x_out) #, dropout = 0.5\n",
    "\n",
    "\n",
    "# MLP -- Regression\n",
    "predictions = layers.Dense(units = train_targets.shape[1], activation = \"linear\")(x_out) \n",
    "# len(x_inp), x_out[1:].shape\n",
    "train_targets.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hard coded size adjustments\n",
    "test_subjects_adjusted = test_subjects[:len(val_subjects)]\n",
    "\n",
    "val_gen = generator.flow(val_subjects.index, test_subjects_adjusted)\n",
    "#train_gen[1], val_gen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions: https://keras.io/api/losses/\n",
    "\n",
    "model = Model(\n",
    "    inputs = x_inp, outputs = predictions)\n",
    "\n",
    "'''\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.1),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# REF: https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay\n",
    "train_steps = 1000\n",
    "lr_fn = optimizers.schedules.PolynomialDecay(1e-3, train_steps, 1e-5, 2)\n",
    "\n",
    "# https://keras.io/api/metrics/\n",
    "model.compile(\n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam( lr_fn ),\n",
    "    # metrics = ['mse']\n",
    "    metrics=['mse', 'mape', 'mae']\n",
    ")\n",
    "'''\n",
    "\n",
    "# 1st block\n",
    "# mape: https://towardsdatascience.com/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac\n",
    "model.compile( \n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam(learning_rate = 0.015), \n",
    "    #optimizer = optimizers.Adam(lr_fn), \n",
    "    # metrics=['mse']\n",
    "    metrics=['mse', 'mape', 'mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_inp), predictions.shape, print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_subjects)\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded size adjustments\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "val_gen = generator.flow(val_subjects.index, test_subjects_)\n",
    "#train_gen[1], val_gen[1]\n",
    "val_gen[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_gen[:1][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_gen_data), type(data_valid), type(x_inp), type(x_out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "epochs_to_test = 10000\n",
    "patience_to_test = 10000\n",
    "\n",
    "es_callback = EarlyStopping(\n",
    "    monitor = \"val_mean_squared_error\", \n",
    "    patience = patience_to_test, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "data_valid = val_gen #[:1][:4];\n",
    "train_gen_data = train_gen #[:1][:4];\n",
    "\n",
    "history = model.fit( train_gen_data, epochs = epochs_to_test, validation_data = data_valid, verbose = 2,    \n",
    "    # shuffling = true means shuffling the whole graph\n",
    "    shuffle = False, callbacks = [es_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subjects, \n",
    "test_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Metrics for Pearson Based Prediction: GCN + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "train_metrics = model.evaluate(train_gen)\n",
    "print(\"\\nTrain Set Metrics:\")\n",
    "\n",
    "print(\"Train Metrics for Pearson Based Prediction: GCN + CNN\");\n",
    "for name, val in zip(model.metrics_names, train_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator.flow(test_subjects.index, test_targets)\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(columns=['Method', 'Loss', 'MSE', 'MAPE', 'MAE'])\n",
    "\n",
    "temp = list()\n",
    "temp.append('GCN-Pearson');\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    # print(val)\n",
    "    temp.append(round(val,2))\n",
    "\n",
    "print(temp)\n",
    "df_metrics.loc[1] = temp\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the predicted prices by the Model\n",
    "\n",
    "At this point, I still need to make sense of what GCN ( and CNN) combination + MLP is predicting. \n",
    "I am just displaying the output. \n",
    "It appears that price is predicted for each timestamp (day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = pearson_graph_node_data.index;\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = model.predict(all_gen)\n",
    "\n",
    "all_nodes, all_predictions, all_predictions.shape, pearson_graph_node_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\n",
    "model.predict(\n",
    "    all_gen,\n",
    "    batch_size = None,\n",
    "    verbose = 2,\n",
    "    steps = None,\n",
    "    callbacks = None,\n",
    "    max_queue_size = 10,\n",
    "    workers = 1,\n",
    "    use_multiprocessing = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions = model.predict(all_nodes)\n",
    "\n",
    "# all_predictions, all_predictions.shape, pearson_graph_node_data.shape\n",
    "vars(all_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_graph_node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(all_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen[:1][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************************************\n",
    "STOP because we are testing a new model\n",
    "****************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEARMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman\n",
    "# epochs_to_test = 15000\n",
    "# patience_to_test = 15000\n",
    "\n",
    "df_s_transpose_spearman = df_s_transpose.corr(method = 'spearman', numeric_only = True)\n",
    "df_s_transpose_spearman\n",
    "\n",
    "\n",
    "# # Pearson Correlation Coefficient based Adjacency Graph Matrix\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "df_s_transpose_spearman[df_s_transpose_spearman >= 0.4] = 1\n",
    "df_s_transpose_spearman[df_s_transpose_spearman < 0.4] = 0\n",
    "df_s_transpose_spearman\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "# make the diagonal element to be zero. No self loop\n",
    "import numpy as np\n",
    "np.fill_diagonal(df_s_transpose_spearman.values, 0)\n",
    "df_s_transpose_spearman\n",
    "\n",
    "\n",
    "# Create and visualize the Graphs\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "Graph_spearman = nx.Graph(df_s_transpose_spearman)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "nx.draw_networkx(Graph_spearman, pos=nx.circular_layout(Graph_spearman), node_color='r', edge_color='b')\n",
    "\n",
    "\n",
    "# # Create GCN layer. Graph_spearman\n",
    "\n",
    "# # Find all stocks = nodes\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# improvement: make sure only stocks/nodes that are in the graph are taken\n",
    "all_stock_nodes = df_s_transpose_spearman.index.to_list()\n",
    "all_stock_nodes\n",
    "\n",
    "\n",
    "# # Find all edges between nodes\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "source = [];\n",
    "target = [];\n",
    "edge_feature = [];\n",
    "\n",
    "for aStock in all_stock_nodes:\n",
    "    for anotherStock in all_stock_nodes:\n",
    "        if df_s_transpose_spearman[aStock][anotherStock] > 0:\n",
    "            #print(df_s_transpose_spearman[aStock][anotherStock])\n",
    "            source.append(aStock)\n",
    "            target.append(anotherStock)\n",
    "            edge_feature.append(1)\n",
    "            \n",
    "source, target, edge_feature            \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "# https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-pandas.html\n",
    "spearman_edges = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target}\n",
    ")\n",
    "\n",
    "spearman_edges_data = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target, \"edge_feature\": edge_feature}\n",
    ")\n",
    "\n",
    "\n",
    "spearman_edges[:10]\n",
    "\n",
    "\n",
    "# # Graph with No Feature Data, No node data, only edges\n",
    "\n",
    "# spearman_graph = StellarGraph(edges = spearman_edges, node_type_default=\"corner\", edge_type_default=\"line\")\n",
    "# #spearman_graph = StellarGraph(nodes = all_stock_nodes, edges = spearman_edges)\n",
    "# # graph = sg.StellarGraph(all_stock_nodes, square_edges)\n",
    "# print(spearman_graph.info())\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "# Trying to have the time series data as part of the nodes\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "df_s_transpose\n",
    "\n",
    "\n",
    "# # Structure the Feature Matrix so that it can be passed to the GCN\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "df_s_transpose_feature = df_s_transpose.reset_index(drop = True, inplace = False)\n",
    "# df_s_transpose_feature =  df_s_transpose_feature.values.tolist()\n",
    "# print(df_s_transpose_feature.values.tolist())\n",
    "#df_s_transpose_feature['WY'].values\n",
    "df_s_transpose_feature['AAPL'].values\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "node_Data = [];\n",
    "for x in all_stock_nodes:\n",
    "    node_Data.append( df_s_transpose_feature[x].values)\n",
    "    \n",
    "    \n",
    "node_Data    \n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "spearman_graph_node_data = pd.DataFrame(node_Data, index = all_stock_nodes)\n",
    "spearman_graph_node_data\n",
    "\n",
    "\n",
    "# # Graph with feature as part of Nodes\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "spearman_graph_with_node_features = StellarGraph(spearman_graph_node_data, edges = spearman_edges, node_type_default = \"corner\", edge_type_default = \"line\")\n",
    "print(pearson_graph_with_node_features.info())\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "# Generator\n",
    "generator = FullBatchNodeGenerator(spearman_graph_with_node_features, method = \"gcn\") # , sparse = False\n",
    "vars(generator)\n",
    "\n",
    "\n",
    "# # Train Test Split\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    spearman_graph_node_data #, train_size = 6, test_size = 4\n",
    ")\n",
    "# , train_size=6, test_size=None, stratify=pearson_graph_node_data\n",
    "\n",
    "val_subjects, test_subjects_step_2 = model_selection.train_test_split(\n",
    "    test_subjects #, test_size = 2\n",
    ")\n",
    "\n",
    "#, train_size = 500, test_size = None, stratify = test_subjects\n",
    "\n",
    "\n",
    "train_subjects.shape, test_subjects.shape, val_subjects.shape, test_subjects_step_2.shape\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "spearman_graph_node_data\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "train_targets = train_subjects; \n",
    "val_targets = val_subjects; \n",
    "test_targets = test_subjects; \n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "# debug\n",
    "train_subjects.index, \n",
    "train_targets\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# train data size\n",
    "# it is not must to use a number like unit_count\n",
    "unit_count = train_subjects.shape[0]\n",
    "unit_count\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "'''\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow import keras\n",
    "\n",
    "layer_sizes = [32, 32]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "'''\n",
    "\n",
    "gcn = GCN(layer_sizes = layer_sizes, activations = activations, generator = generator) #, dropout = 0.5\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "# MLP -- Regression\n",
    "predictions = layers.Dense(units = train_targets.shape[1], activation = \"linear\")(x_out)\n",
    "\n",
    "'''\n",
    "x_out, \n",
    "x_inp, x_out\n",
    "'''\n",
    "\n",
    "# # hard coded size adjustments\n",
    "# test_subjects_adjusted = test_subjects[:len(val_subjects)]\n",
    "# \n",
    "# val_gen = generator.flow(val_subjects.index, test_subjects_adjusted)\n",
    "# # train_gen[1], val_gen[1]\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "# Models Although this code could be removed as Model is defined earlier and the same model/architecture is used by all approaches\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "# loss functions: https://keras.io/api/losses/\n",
    "'''\n",
    "model = Model(\n",
    "    inputs = x_inp, outputs = predictions\n",
    ")\n",
    "'''\n",
    "'''\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.1),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "'''\n",
    "\n",
    "# REF: https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay\n",
    "# train_steps = 1000\n",
    "# lr_fn = optimizers.schedules.PolynomialDecay(1e-3, train_steps, 1e-5, 2)\n",
    "\n",
    "\n",
    "# https://keras.io/api/metrics/\n",
    "'''\n",
    "model.compile(\n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam( lr_fn ),\n",
    "    # metrics = ['mean_squared_error']\n",
    "    metrics=['mse', 'mape', 'mae]\n",
    ")\n",
    "'''\n",
    "# 2nd block\n",
    "# mape: https://towardsdatascience.com/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac\n",
    "model.compile( \n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam(learning_rate = 0.015), \n",
    "    #optimizer = optimizers.Adam(lr_fn), \n",
    "    # metrics=['mean_squared_error']\n",
    "    metrics=['mean_squared_error', 'mape', 'mae']\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "len(x_inp), predictions.shape, print(model.summary())\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "len(val_subjects)\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "# hard coded size adjustments\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "val_gen = generator.flow(val_subjects.index, test_subjects_)\n",
    "#train_gen[1], val_gen[1]\n",
    "\n",
    "\n",
    "# train_gen[:1][:4]\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "'''\n",
    "#epochs_to_test = 10000\n",
    "#patience_to_test = 10000\n",
    "\n",
    "es_callback = EarlyStopping(\n",
    "    monitor = \"val_mean_squared_error\", \n",
    "    patience = patience_to_test, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "data_valid = val_gen #[:1][:4];\n",
    "train_gen_data = train_gen #[:1][:4];\n",
    "'''\n",
    "\n",
    "history = model.fit( train_gen_data, epochs = epochs_to_test, validation_data = data_valid, verbose = 2,    \n",
    "    # shuffling = true means shuffling the whole graph\n",
    "    shuffle = False, callbacks = [es_callback],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "val_subjects, \n",
    "test_subjects\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "train_metrics = model.evaluate(train_gen)\n",
    "print(\"\\nTrain Set Metrics:\")\n",
    "\n",
    "print(\"Train Metrics for Spearman Based Prediction: GCN + CNN\");\n",
    "for name, val in zip(model.metrics_names, train_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "test_gen = generator.flow(test_subjects.index, test_targets)\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "    \n",
    "#df_metrics = pd.DataFrame(columns=['Method', 'Loss', 'MSE', 'MAPE', 'MAE'])\n",
    "\n",
    "temp = list()\n",
    "temp.append('GCN-Spearman');\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    # print(val)\n",
    "    temp.append(round(val,2))\n",
    "\n",
    "print(temp)\n",
    "df_metrics.loc[2] = temp\n",
    "df_metrics\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# # Show the predicted prices by the Model\n",
    "# \n",
    "# At this point, I still need to make sense of what GCN ( and CNN) combination + MLP is predicting. \n",
    "# I am just displaying the output. \n",
    "# It appears that price is predicted for each timestamp (day)\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "all_nodes = spearman_graph_node_data.index;\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = model.predict(all_gen)\n",
    "\n",
    "all_nodes, all_predictions, all_predictions.shape, spearman_graph_node_data.shape\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\n",
    "model.predict(\n",
    "    all_gen,\n",
    "    batch_size = None,\n",
    "    verbose = 2,\n",
    "    steps = None,\n",
    "    callbacks = None,\n",
    "    max_queue_size = 10,\n",
    "    workers = 1,\n",
    "    use_multiprocessing = False\n",
    ")\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "# all_predictions = model.predict(all_nodes)\n",
    "\n",
    "# all_predictions, all_predictions.shape, spearman_graph_node_data.shape\n",
    "vars(all_gen)\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "spearman_graph_node_data\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "vars(all_gen)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "train_gen[:1][:4]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kendal Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kendall_tau\n",
    "\n",
    "#epochs_to_test = 15000\n",
    "#patience_to_test = 15000\n",
    "\n",
    "\n",
    "df_s_transpose_kendall_tau = df_s_transpose.corr(method = 'kendall', numeric_only = True)\n",
    "df_s_transpose_kendall_tau\n",
    "\n",
    "\n",
    "# # kendall_tau Correlation Coefficient based Adjacency Graph Matrix\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "df_s_transpose_kendall_tau[df_s_transpose_kendall_tau >= 0.3] = 1\n",
    "df_s_transpose_kendall_tau[df_s_transpose_kendall_tau < 0.3] = 0\n",
    "df_s_transpose_kendall_tau\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "# make the diagonal element to be zero. No self loop\n",
    "import numpy as np\n",
    "np.fill_diagonal(df_s_transpose_kendall_tau.values, 0)\n",
    "df_s_transpose_kendall_tau\n",
    "\n",
    "\n",
    "# Create and visualize the Graphs\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "Graph_kendall_tau = nx.Graph(df_s_transpose_kendall_tau)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "nx.draw_networkx(Graph_kendall_tau, pos=nx.circular_layout(Graph_kendall_tau), node_color='r', edge_color='b')\n",
    "\n",
    "\n",
    "# # Create GCN layer. Graph_kendall_tau\n",
    "\n",
    "# # Find all stocks = nodes\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "# improvement: make sure only stocks/nodes that are in the graph are taken\n",
    "all_stock_nodes = df_s_transpose_kendall_tau.index.to_list()\n",
    "all_stock_nodes\n",
    "\n",
    "\n",
    "# # Find all edges between nodes\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "source = [];\n",
    "target = [];\n",
    "edge_feature = [];\n",
    "\n",
    "for aStock in all_stock_nodes:\n",
    "    for anotherStock in all_stock_nodes:\n",
    "        if df_s_transpose_kendall_tau[aStock][anotherStock] > 0:\n",
    "            #print(df_s_transpose_kendall_tau[aStock][anotherStock])\n",
    "            source.append(aStock)\n",
    "            target.append(anotherStock)\n",
    "            edge_feature.append(1)\n",
    "            \n",
    "source, target, edge_feature            \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "# https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-pandas.html\n",
    "kendall_tau_edges = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target}\n",
    ")\n",
    "\n",
    "kendall_tau_edges_data = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target, \"edge_feature\": edge_feature}\n",
    ")\n",
    "\n",
    "\n",
    "kendall_tau_edges[:10]\n",
    "\n",
    "\n",
    "# # Graph with No Feature Data, No node data, only edges\n",
    "\n",
    "# kendall_tau_graph = StellarGraph(edges = kendall_tau_edges, node_type_default=\"corner\", edge_type_default=\"line\")\n",
    "# #kendall_tau_graph = StellarGraph(nodes = all_stock_nodes, edges = kendall_tau_edges)\n",
    "# # graph = sg.StellarGraph(all_stock_nodes, square_edges)\n",
    "# print(kendall_tau_graph.info())\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "# Trying to have the time series data as part of the nodes\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "df_s_transpose\n",
    "\n",
    "\n",
    "# # Structure the Feature Matrix so that it can be passed to the GCN\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "df_s_transpose_feature = df_s_transpose.reset_index(drop = True, inplace = False)\n",
    "# df_s_transpose_feature =  df_s_transpose_feature.values.tolist()\n",
    "# print(df_s_transpose_feature.values.tolist())\n",
    "#df_s_transpose_feature['WY'].values\n",
    "df_s_transpose_feature['AAPL'].values\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "node_Data = [];\n",
    "for x in all_stock_nodes:\n",
    "    node_Data.append( df_s_transpose_feature[x].values)\n",
    "    \n",
    "    \n",
    "node_Data    \n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "kendall_tau_graph_node_data = pd.DataFrame(node_Data, index = all_stock_nodes)\n",
    "kendall_tau_graph_node_data\n",
    "\n",
    "\n",
    "# # Graph with feature as part of Nodes\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "kendall_tau_graph_with_node_features = StellarGraph(kendall_tau_graph_node_data, edges = kendall_tau_edges, node_type_default = \"corner\", edge_type_default = \"line\")\n",
    "print(kendall_tau_graph_with_node_features.info())\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "# Generator\n",
    "generator = FullBatchNodeGenerator(kendall_tau_graph_with_node_features, method = \"gcn\") # , sparse = False\n",
    "vars(generator)\n",
    "\n",
    "\n",
    "# # Train Test Split\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    kendall_tau_graph_node_data #, train_size = 6, test_size = 4\n",
    ")\n",
    "# , train_size=6, test_size=None, stratify=kendall_tau_graph_node_data\n",
    "\n",
    "val_subjects, test_subjects_step_2 = model_selection.train_test_split(\n",
    "    test_subjects #, test_size = 2\n",
    ")\n",
    "\n",
    "#, train_size = 500, test_size = None, stratify = test_subjects\n",
    "\n",
    "\n",
    "train_subjects.shape, test_subjects.shape, val_subjects.shape, test_subjects_step_2.shape\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "kendall_tau_graph_node_data\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "train_targets = train_subjects; \n",
    "val_targets = val_subjects; \n",
    "test_targets = test_subjects; \n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "# debug\n",
    "train_subjects.index, \n",
    "train_targets\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# train data size\n",
    "# it is not must to use a number like unit_count\n",
    "unit_count = train_subjects.shape[0]\n",
    "unit_count\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "'''\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow import keras\n",
    "\n",
    "layer_sizes = [32, 32]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "'''\n",
    "gcn = GCN(layer_sizes = layer_sizes, activations = activations, generator = generator) #, dropout = 0.5\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "# MLP -- Regression\n",
    "predictions = layers.Dense(units = train_targets.shape[1], activation = \"linear\")(x_out)\n",
    "\n",
    "'''\n",
    "x_out, \n",
    "x_inp, x_out\n",
    "\n",
    "\n",
    "# # hard coded size adjustments\n",
    "# test_subjects_adjusted = test_subjects[:len(val_subjects)]\n",
    "# \n",
    "# val_gen = generator.flow(val_subjects.index, test_subjects_adjusted)\n",
    "# # train_gen[1], val_gen[1]\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "# Models\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "# loss functions: https://keras.io/api/losses/\n",
    "\n",
    "model = Model(\n",
    "    inputs = x_inp, outputs = predictions)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.1),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "\n",
    "# REF: https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay\n",
    "train_steps = 1000\n",
    "lr_fn = optimizers.schedules.PolynomialDecay(1e-3, train_steps, 1e-5, 2)\n",
    "\n",
    "\n",
    "# https://keras.io/api/metrics/\n",
    "model.compile(\n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam( lr_fn ),\n",
    "    # metrics = ['mean_squared_error']\n",
    "    metrics=['mse', 'mape',  'mae']\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "# 3rd block\n",
    "# mape: https://towardsdatascience.com/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac\n",
    "model.compile( \n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam(learning_rate = 0.015), \n",
    "    #optimizer = optimizers.Adam(lr_fn), \n",
    "    # metrics=['mean_squared_error']\n",
    "    metrics=['mean_squared_error', 'mape', 'mae']\n",
    "    # metrics=[\n",
    "    #    metrics.MeanSquaredError(),\n",
    "    #    metrics.AUC(),\n",
    "    #]\n",
    ")\n",
    "\n",
    "\n",
    "len(x_inp), predictions.shape, print(model.summary())\n",
    "\n",
    "len(val_subjects)\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "# hard coded size adjustments\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "val_gen = generator.flow(val_subjects.index, test_subjects_)\n",
    "#train_gen[1], val_gen[1]\n",
    "\n",
    "\n",
    "# train_gen[:1][:4]\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "'''\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es_callback = EarlyStopping(\n",
    "    monitor = \"val_mean_squared_error\", \n",
    "    patience = patience_to_test, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "data_valid = val_gen #[:1][:4];\n",
    "train_gen_data = train_gen #[:1][:4];\n",
    "'''\n",
    "\n",
    "history = model.fit( train_gen_data, epochs = epochs_to_test, validation_data = data_valid, verbose = 2,    \n",
    "    # shuffling = true means shuffling the whole graph\n",
    "    shuffle = False, callbacks = [es_callback],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(Graph_kendall_tau, pos=nx.circular_layout(Graph_kendall_tau), node_color='r', edge_color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "val_subjects, \n",
    "test_subjects\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "train_metrics = model.evaluate(train_gen)\n",
    "print(\"\\nTrain Set Metrics:\")\n",
    "\n",
    "print(\"Train Metrics for Kendall Tau Based Prediction: GCN + CNN\");\n",
    "for name, val in zip(model.metrics_names, train_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "\n",
    "\n",
    "test_gen = generator.flow(test_subjects.index, test_targets)\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "\n",
    "\n",
    "# # Show the predicted prices by the Model\n",
    "# \n",
    "# At this point, I still need to make sense of what GCN ( and CNN) combination + MLP is predicting. \n",
    "# I am just displaying the output. \n",
    "# It appears that price is predicted for each timestamp (day)\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "all_nodes = kendall_tau_graph_node_data.index;\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = model.predict(all_gen)\n",
    "\n",
    "all_nodes, all_predictions, all_predictions.shape, kendall_tau_graph_node_data.shape\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\n",
    "model.predict(\n",
    "    all_gen,\n",
    "    batch_size = None,\n",
    "    verbose = 2,\n",
    "    steps = None,\n",
    "    callbacks = None,\n",
    "    max_queue_size = 10,\n",
    "    workers = 1,\n",
    "    use_multiprocessing = False\n",
    ")\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "# all_predictions = model.predict(all_nodes)\n",
    "\n",
    "# all_predictions, all_predictions.shape, kendall_tau_graph_node_data.shape\n",
    "vars(all_gen)\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "kendall_tau_graph_node_data\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "vars(all_gen)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "train_gen[:1][:4]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metrics = pd.DataFrame(columns=['Method', 'Loss', 'MSE', 'MAPE', 'MAE'])\n",
    "# df_metrics = pd.DataFrame(columns=['Method', 'Loss', 'MSE', 'MAPE', 'MAE'])\n",
    "\n",
    "temp = list()\n",
    "temp.append('GCN-Kendall');\n",
    "for name, val in zip(model.metrics_names, test_metrics):    \n",
    "    temp.append(round(val,2))\n",
    "\n",
    "print(temp)\n",
    "df_metrics.loc[3] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "df_metrics_plot = df_metrics[['Loss', 'MSE', 'MAPE', 'MAE']]\n",
    "\n",
    "#temp = [10.71573, 13.578422, 10.71573, 16.638063]\n",
    "#temp = [19.04899024963379, 1377.4075927734375, 19.04899024963379, 26.09033203125]\n",
    "#df_metrics_plot.loc[4] = temp\n",
    "\n",
    "df_metrics_plot['MSE'] = [ math.sqrt(x) for x in df_metrics_plot['MSE']];\n",
    "df_metrics_plot\n",
    "#df_metrics, df_metrics_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_plot.plot( kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the sake of easier execution, I have brought financial news based prediction in the same code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Import Libraries\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#epochs_to_test = 15000\n",
    "#patience_to_test = 15000\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# Import Libraries for Graph, GNN, and GCN\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Machine Learnig related library Imports\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# was active\n",
    "\n",
    "data_folder = './data/yahoonewsarchive/'\n",
    "# os.chdir(data_folder);\n",
    "# file = data_folder + 'NEWS_YAHOO_stock_prediction.csv';\n",
    "file = data_folder + 'News_Yahoo_stock.csv';\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df_news = pd.read_csv(file)\n",
    "df_news.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df_news = df_news[:100]\n",
    "\n",
    "\n",
    "# # Approaches: Find all stock tickers in an/all article/articles\n",
    "# \n",
    "# 1. Find code that does this: from internet or from previous work or from courses that you have taken online or in academia\n",
    "# 2. Iterative read the article and match with stock tickers, and find all tickers. Drawback: to which tickers to match or how will you know what is a ticker? Any two to four letters Uppercase, NASDAQ AAPL\n",
    "# 3. Load the article in database and then use SQL -> may not work that well unless you write some functions\n",
    "# 4. NLTK, remove stop words, find all tokens, then find All Uppercase words. create a list. attach article ids to the list. Then match with the list of tockers. find common tickers between them. then create tuples with two (indicating edge) (source target weight) \n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# import NLTK libraries\n",
    "# remove stop words using NLTK methods \n",
    "# remove all sorts of unnecessary words\n",
    "# find all tokens\n",
    "# Keep only All Uppercase words in a list : dictionary/map: dataframe will be ideal\n",
    "# create a list/dictionary/map: dataframe will be ideal. attach article ids to the list/dataframe data.\n",
    "# Create a list of all NasDAQ Tickers\n",
    "# Then match with the list of NASDAQ tockers. \n",
    "# find common tickers between them. \n",
    "# then create tuples with two (indicating edge) (source target weight)\n",
    "# increase weight for each article and pair when you see a match\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# import NLTK libraries\n",
    "import nltk\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# remove stop words using NLTK methods \n",
    "# remove all sorts of unnecessary words\n",
    "# find all tokens\n",
    "# Keep only All Uppercase words in a list : dictionary/map: dataframe will be ideal\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "dataFrameWithOnlyCapitalWords = pd.DataFrame(columns =  [\"id\", \"Title\", \"Content\"]) \n",
    "for index, row in df_news.iterrows():\n",
    "    # print(row[id], row['title'], row['content'])\n",
    "                \n",
    "    # words with capital letters in the beginning +  as much as possible\n",
    "    capitalWords = RegexpTokenizer('[A-Z]+[A-Z]\\w+')\n",
    "    # print(\"\\n::All Capital Words::\", capitalWords.tokenize(row['content']))\n",
    "    allCapitalWords = capitalWords.tokenize(row['content'])\n",
    "        \n",
    "    dataFrameWithOnlyCapitalWords.loc[index] = [index, row['title'], allCapitalWords]\n",
    "    #break\n",
    "\n",
    "\n",
    "\n",
    "dataFrameWithOnlyCapitalWords.head() #, dataFrameWithOnlyCapitalWords.shape\n",
    "\n",
    "\n",
    "# # Create a list of all (NasDAQ) 30 stocks as per the paper\n",
    "# \n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# Find/Create a list of NASDAQ Stocks\n",
    "import os\n",
    "import glob\n",
    "nasdaqDataFolder = './archive/stock_market_data/nasdaq/csv'\n",
    "os.chdir(nasdaqDataFolder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Create a list of all NasDAQ Tickers\n",
    "\n",
    "extension = \"csv\"\n",
    "fileTypesToMerge = \"\"\n",
    "# all_filenames = [i for i in glob.glob('*' + '*.{}'.format(extension))]\n",
    "all_nasdaq_tickers = [i[:-4] for i in glob.glob('*' + fileTypesToMerge + '*.{}'.format(extension))]\n",
    "nasdaq_tickers_to_process = all_nasdaq_tickers #[:10]\n",
    "nasdaq_tickers_to_process\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "nasdaq_tickers_to_process.remove('FREE')\n",
    "nasdaq_tickers_to_process.remove('CBOE')\n",
    "nasdaq_tickers_to_process.remove('III')\n",
    "nasdaq_tickers_to_process.remove('RVNC')\n",
    "sorted(nasdaq_tickers_to_process)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "fortune_30_tickers_to_process = [\n",
    "'WMT',\n",
    "'XOM',\n",
    "'AAPL',\n",
    "'UNH',\n",
    "'MCK',\n",
    "'CVS',\n",
    "'AMZN',\n",
    "'T',\n",
    "'GM',\n",
    "'F',\n",
    "'ABC',\n",
    "'CVX',\n",
    "'CAH',\n",
    "'COST',\n",
    "'VZ',\n",
    "'KR',\n",
    "'GE',\n",
    "'WBA',\n",
    "'JPM',\n",
    "'GOOGL',\n",
    "'HD',\n",
    "'BAC',\n",
    "'WFC',\n",
    "'BA',\n",
    "'PSX',\n",
    "'ANTM',\n",
    "'MSFT',\n",
    "'UNP',\n",
    "'PCAR',\n",
    "'DWDP']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nasdaq_tickers_to_process = [\n",
    "# 'WMT',\n",
    "# 'XOM',\n",
    "# 'AAPL',\n",
    "# 'UNH',\n",
    "# 'MCK',\n",
    "# 'CVS',\n",
    "# 'AMZN',\n",
    "# 'T',\n",
    "# 'GM',\n",
    "# 'F',\n",
    "# 'ABC',\n",
    "# 'CVX',\n",
    "# 'CAH',\n",
    "# 'COST',\n",
    "# 'VZ',\n",
    "# 'KR',\n",
    "# 'GE',\n",
    "# 'WBA',\n",
    "# # 'JPM',\n",
    "# #'GOOGL',\n",
    "# 'HD',\n",
    "# 'BAC',\n",
    "# 'WFC',\n",
    "# 'BA',\n",
    "# 'PSX',\n",
    "# 'ANTM',\n",
    "# 'MSFT',\n",
    "# 'UNP',\n",
    "# 'PCAR',\n",
    "# 'DWDP']\n",
    "# \n",
    "\n",
    "# # Find NASDQ Tickers in each article\n",
    "# Create graph steps\n",
    "# Find all edges \n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "combinedTupleList = [];\n",
    "allMatchingTickers = [];\n",
    "from itertools import combinations\n",
    "for index, row in dataFrameWithOnlyCapitalWords.iterrows():\n",
    "    #print(index)\n",
    "    #print(set(row['Content']))\n",
    "    #print(set(nasdaq_tickers_to_process));    \n",
    "    matchingTickers = set(set(fortune_30_tickers_to_process).intersection(set(row['Content'])))\n",
    "    #print(matchingTickers)\n",
    "    if len (matchingTickers) > 1:\n",
    "        allTuples = list(combinations(matchingTickers, 2));\n",
    "        #print(list(combinations(matchingTickers, 2)))\n",
    "        \n",
    "        #allMatchingTickers = set(allMatchingTickers).union(matchingTickers);\n",
    "        for aTuple in allTuples:\n",
    "            combinedTupleList.append(tuple(sorted(aTuple)));\n",
    "            allMatchingTickers.append(aTuple[0])\n",
    "            allMatchingTickers.append(aTuple[1])\n",
    "            \n",
    "        \n",
    "    # print(\"*******************\");\n",
    "    #break\n",
    "    \n",
    "#combinedTupleList = list(set(combinedTupleList))\n",
    "allMatchingTickers = set(allMatchingTickers)\n",
    "\n",
    "# list(set(combinedTupleList)), len(combinedTupleList), len(set(combinedTupleList)), allMatchingTickers, len(allMatchingTickers), len(set(allMatchingTickers))\n",
    "sorted(combinedTupleList), type(aTuple), type(sorted(aTuple)), allMatchingTickers\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "#combinedTupleList[:1], set(allMatchingTickers)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# calculate edge weights\n",
    "from collections import Counter\n",
    "\n",
    "tuplesWithCount = dict(Counter(combinedTupleList))\n",
    "tuplesWithCount\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "l = list(tuplesWithCount.keys())\n",
    "l\n",
    "\n",
    "#print(list(zip(*l))[0])\n",
    "#print(list(zip(*l))[1])\n",
    "\n",
    "source = list(zip(*l))[0];\n",
    "target = list(zip(*l))[1];\n",
    "edge_weights = tuplesWithCount.values()\n",
    "source, target, edge_weights, len(source), len(target)\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "Graph_news = nx.Graph(tuplesWithCount.keys())\n",
    "nx.draw_networkx(Graph_news, pos = nx.circular_layout(Graph_news), node_color = 'r', edge_color = 'b')\n",
    "#tuplesWithCount.keys()\n",
    "\n",
    "\n",
    "# # Finally Create graph based on financial news\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('../../../../')\n",
    "#os.chdir('./mcmaster/meng/747/project/')\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "# Now create node data i.e time series to pass as part of the nodes\n",
    "'''\n",
    "df = pd.DataFrame();\n",
    "data_file = \"../../../..//archive/stock_market_data/nasdaq/nasdq-stock-price--all-merged.csv\"\n",
    "# stock-price--all-merged.csv\"\n",
    "df = pd.read_csv(data_file);\n",
    "df.head()\n",
    "'''\n",
    "\n",
    "# this is the place where the new dataset starts i.e. fortune 30 companies\n",
    "df = pd.DataFrame();\n",
    "data_file = \"per-day-fortune-30-company-stock-price-data.csv\";\n",
    "df = pd.read_csv(\"./data/\" + data_file, low_memory = False);\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "df.index\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "drop_cols_with_na = 1\n",
    "drop_rows_with_na = 0\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\n",
    "\n",
    "\n",
    "try:\n",
    "  df = df.interpolate(inplace = False)\n",
    "except:\n",
    "  print(\"An exception occurred. Operation ignored\")\n",
    "  exit\n",
    "    \n",
    "df.isnull().values.any()\n",
    "df[df.isna().any(axis = 1)]  \n",
    "\n",
    "\n",
    "#----\n",
    "\n",
    "\n",
    "\n",
    "if drop_cols_with_na == 1:\n",
    "    df = df.dropna(axis = 1);    \n",
    "   \n",
    "df, df.shape\n",
    "\n",
    "## -- \n",
    "\n",
    "df.isnull().values.any()\n",
    "df[df.isna().any( axis = 1 )]\n",
    "\n",
    "\n",
    "## --\n",
    "\n",
    "# df_s_transpose.index = df_s_transpose['Date']\n",
    "#df.index = df.index.astype('datetime64[ns]')\n",
    "df\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "df_s =  df #[ ['Ticker', 'Date', 'Adjusted Close'] ];\n",
    "df_s\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "df_s[\"Date\"] = df_s[\"Date\"].astype('datetime64[ns]')\n",
    "df_s = df_s.sort_values( by = 'Date', ascending = True )\n",
    "df_s\n",
    "\n",
    "\n",
    "# df_s_pivot = df_s.pivot_table(index = 'Ticker', columns = 'Date', values = 'Adjusted Close')\n",
    "# df_s_pivot\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "allMatchingTickers\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# drop_rows_with_na = 0\n",
    "# if drop_rows_with_na == 1:\n",
    "#     df_s_transpose = df_s_transpose.dropna(axis=0);\n",
    "#     #df_s_transpose[\"Date\"] = df_s_transpose[\"Date\"].astype('datetime64[ns]')\n",
    "#     #df_s_transpose.sort_values(by='Date', ascending=False)\n",
    "#     df_s_transpose.to_csv('../../../..//archive/stock_market_data/nasdaq/-na-dropped-nasdq-stock-price--all-merged.csv');\n",
    "#    \n",
    "# df_s_transpose.head(100)\n",
    "# \n",
    "# \n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "df_s_transpose = df_s #_pivot.T\n",
    "df_s_transpose\n",
    "\n",
    "df_s_transpose_feature = df_s_transpose.reset_index(drop = True, inplace=False)\n",
    "# df_s_transpose_feature =  df_s_transpose_feature.values.tolist()\n",
    "# print(df_s_transpose_feature.values.tolist())\n",
    "#df_s_transpose_feature['AAPL'].values\n",
    "\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "df_s_transpose_feature = df_s_transpose.set_index('Date')\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "df_s_transpose_feature\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "#df_s_transpose['SIMO']\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# df_s_transpose_feature['AAPL'].values\n",
    "len(allMatchingTickers), len(set(allMatchingTickers)) #, df_s['Ticker']\n",
    "#df_s_tickers = df_s['Ticker'];\n",
    "#len(df_s_tickers), len(set(allMatchingTickers)), df_s_transpose.columns.unique, len(set(df_s['Ticker']))\n",
    "#df_s_tickers = list(set(df_s['Ticker'])); # list(df_s_transpose.columns.unique) #\n",
    "#sorted(df_s_tickers)\n",
    "#for x in df_s_tickers:\n",
    " #   print(x)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "df_s_tickers = df_s_transpose_feature.columns\n",
    "#df_s_tickers = list(set(df_s_tickers.drop('Date')))\n",
    "sorted(df_s_tickers[:5])\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "set_allMatchingTickers = set(allMatchingTickers)\n",
    "df_s_tickers = fortune_30_tickers_to_process #list(set(df_s['Ticker'])); # list(df_s_transpose.columns.unique) #\n",
    "node_Data_financial_news = [];\n",
    "\n",
    "'''\n",
    "for x in set_allMatchingTickers :\n",
    "    # if x in df_s_tickers:\n",
    "    print(x)\n",
    "    node_Data_financial_news.append( df_s_transpose_feature[x].values)\n",
    "'''  \n",
    "\n",
    "node_Data_financial_news = pd.DataFrame(df_s_transpose_feature) #, index = list(allMatchingTickers)) #, index = list(set_allMatchingTickers))\n",
    "#node_Data_financial_news = node_Data_financial_news.T \n",
    "node_Data_financial_news\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "node_Data_financial_news = node_Data_financial_news.T\n",
    "node_Data_financial_news\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "node_Data_financial_news\n",
    "\n",
    "\n",
    "# node_Data_financial_news#.drop(axis = 0)\n",
    "# node_Data_financial_news = node_Data_financial_news.T\n",
    "# node_Data_financial_news\n",
    "\n",
    "# node_Data_financial_news = node_Data_financial_news.drop('Date')\n",
    "# node_Data_financial_news\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "financial_news_edge_data = pd.DataFrame(\n",
    "    {\"source\": source, \"target\": target, \"edge_feature\": edge_weights}\n",
    ")\n",
    "\n",
    "financial_news_graph = StellarGraph(node_Data_financial_news, edges = financial_news_edge_data, node_type_default=\"corner\", edge_type_default=\"line\")\n",
    "print(financial_news_graph.info())\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "# debug code\n",
    "# financial_news_graph_data,  sorted(node_Data_financial_news.columns.unique())\n",
    "# [1,2] + [2, 3,4], set(source + target).difference(sorted(node_Data_financial_news.columns.unique()))\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "# Generator\n",
    "generator = FullBatchNodeGenerator(financial_news_graph, method = \"gcn\")\n",
    "\n",
    "\n",
    "# # Machine Learning, Deep Learning, GCN, CNN\n",
    "\n",
    "# # Train Test Split\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    node_Data_financial_news #, train_size = 6, test_size = 4\n",
    ")\n",
    "# , train_size=6, test_size=None, stratify=pearson_graph_node_data\n",
    "\n",
    "val_subjects, test_subjects_step_2 = model_selection.train_test_split(\n",
    "    test_subjects #, test_size = 2\n",
    ")\n",
    "\n",
    "#, train_size = 500, test_size = None, stratify = test_subjects\n",
    "\n",
    "\n",
    "train_subjects.shape, test_subjects.shape, val_subjects.shape, test_subjects_step_2.shape\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "# just the target variables\n",
    "\n",
    "train_targets = train_subjects; \n",
    "val_targets = val_subjects; \n",
    "test_targets = test_subjects; \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "# Architecture of the Neural Network\n",
    "train_subjects.index, train_targets\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "'''\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow import keras\n",
    "\n",
    "layer_sizes = [32, 32]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "'''\n",
    "\n",
    "gcn = GCN(layer_sizes = layer_sizes, activations = activations, generator = generator) #, dropout = 0.5\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "# MLP -- Regression\n",
    "predictions = layers.Dense(units = train_targets.shape[1], activation = \"linear\")(x_out)\n",
    "\n",
    "\n",
    "'''\n",
    "x_out, \n",
    "x_inp, x_out\n",
    "\n",
    "\n",
    "# # Models\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "# loss functions: https://keras.io/api/losses/\n",
    "\n",
    "model = Model(\n",
    "    inputs = x_inp, outputs = predictions)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.1),\n",
    "    loss=losses.MeanSquaredError(),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "\n",
    "# REF: https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay\n",
    "train_steps = 1000\n",
    "lr_fn = optimizers.schedules.PolynomialDecay(1e-3, train_steps, 1e-5, 2)\n",
    "\n",
    "\n",
    "# https://keras.io/api/metrics/\n",
    "model.compile(\n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam( lr_fn ),\n",
    "    # metrics = ['mean_squared_error']\n",
    "    metrics=['mse', 'mape',  'mae']\n",
    ")\n",
    "'''\n",
    "# 4th block\n",
    "\n",
    "# mape: https://towardsdatascience.com/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac\n",
    "model.compile( \n",
    "    loss = 'mean_absolute_error', \n",
    "    optimizer = optimizers.Adam(learning_rate = 0.015), \n",
    "    #optimizer = optimizers.Adam(lr_fn), \n",
    "    # metrics=['mean_squared_error']\n",
    "    metrics=['mean_squared_error', 'mape', 'mae']\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "len(x_inp), predictions.shape, print(model.summary())\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "len(val_subjects)\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "# hard coded size adjustments\n",
    "test_subjects_ = test_subjects[:len(val_subjects)]\n",
    "\n",
    "val_gen = generator.flow(val_subjects.index, test_subjects_)\n",
    "#train_gen[1], val_gen[1]\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "\n",
    "data_valid = val_gen #[:1][:4];\n",
    "train_gen_data = train_gen #[:1][:4];\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "type(train_gen_data), type(data_valid), type(x_inp), type(x_out) \n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "'''\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es_callback = EarlyStopping(\n",
    "    monitor = \"val_mean_squared_error\", \n",
    "    patience = patience_to_test, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "'''\n",
    "\n",
    "history = model.fit( train_gen_data, epochs = epochs_to_test, validation_data = data_valid, verbose = 2,    \n",
    "    # shuffling = true means shuffling the whole graph\n",
    "    shuffle = False, callbacks = [es_callback],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "train_metrics = model.evaluate(train_gen)\n",
    "print(\"\\nTrain Set Metrics:\")\n",
    "\n",
    "print(\"Train Metrics for Financial News Based Prediction: GCN + CNN\");\n",
    "for name, val in zip(model.metrics_names, train_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]\n",
    "val_subjects, \n",
    "test_subjects\n",
    "\n",
    "test_gen = generator.flow(test_subjects.index, test_targets)\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "\n",
    "\n",
    "# # Show the predicted prices by the Model\n",
    "# \n",
    "# At this point, I still need to make sense of what GCN ( and CNN) combination + MLP is predicting. \n",
    "# I am just displaying the output. \n",
    "# It appears that price is predicted for each timestamp (day)\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "all_nodes = node_Data_financial_news.index;\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = model.predict(all_gen)\n",
    "\n",
    "all_predictions, all_predictions.shape, node_Data_financial_news.shape\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_metrics = pd.DataFrame(columns=['Method', 'Loss', 'MSE', 'MAPE', 'MAE'])\n",
    "\n",
    "temp = list()\n",
    "temp.append('GCN-Causation-News');\n",
    "for name, val in zip(model.metrics_names, test_metrics):    \n",
    "    temp.append(round(val,2))\n",
    "\n",
    "print(temp)\n",
    "df_metrics.loc[1] = temp\n",
    "\n",
    "import math\n",
    "df_metrics_plot = df_metrics[['Loss', 'MSE', 'MAPE', 'MAE']]\n",
    "df_metrics_plot['MSE'] = math.sqrt(df_metrics['MSE'])\n",
    "df_metrics_plot\n",
    "\n",
    "\n",
    "df_metrics_plot.plot( kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metrics = pd.DataFrame(columns=['Method', 'Loss', 'MSE', 'MAPE', 'MAE'])\n",
    "\n",
    "temp = list()\n",
    "temp.append('GCN-News');\n",
    "for name, val in zip(model.metrics_names, test_metrics):    \n",
    "   temp.append(round(val,2))\n",
    "\n",
    "df_metrics.loc[4] = temp\n",
    "\n",
    "# import math\n",
    "df_metrics_plot = df_metrics[['Loss', 'MSE', 'MAPE', 'MAE']]\n",
    "\n",
    "df_metrics_plot['MSE'] = [ math.sqrt(x) for x in df_metrics_plot['MSE']]\n",
    "df_metrics_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_plot.plot( kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_metrics_plot.T,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, I have taken ideas from the following code esp. to see what GCN is and how GCN works.\n",
    "\n",
    "Although, it does not use any CNN. \n",
    "\n",
    "Node classification with Graph Convolutional Network (GCN). \n",
    "\n",
    "https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7riPbvAKomIR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qw49sErnonN4"
   },
   "source": [
    "References:\n",
    "\n",
    "\n",
    "\n",
    "[1] Node classification with Graph Convolutional Network (GCN). https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html \n",
    "\n",
    "\n",
    "[2] Loading data into StellarGraph from Pandas. https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-pandas.html\n",
    "\n",
    "[3] Load Timeseries https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-numpy.html\n",
    "\n",
    "[4] NetworkX: https://networkx.org/documentation/stable/reference/introduction.html \n",
    "\n",
    "[5]  StellerGraph and Networkx https://stellargraph.readthedocs.io/en/latest/demos/basics/loading-networkx.html \n",
    "\n",
    "[6] Select StellerGraph Algorithm : https://stellargraph.readthedocs.io/en/stable/demos/#find-a-demo-for-an-algorithm \n",
    "[link text](https://)\n",
    "\n",
    "\n",
    "Learning: \n",
    "GNN/GCN/Keras\n",
    "https://www.youtube.com/watch?v=0KH95BEz370\n",
    "\n",
    "\n",
    "Install StellarGraph:\n",
    "https://pypi.org/project/stellargraph/#install-stellargraph-using-pypi\n",
    "\n",
    "\n",
    "May want to use without Stellar\n",
    "https://keras.io/examples/graph/gnn_citations/\n",
    "\n",
    "to get feature data from pandas dataframe: \n",
    "https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-pandas.html\n",
    "\n",
    "\n",
    "Create graph properly:\n",
    "https://stellargraph.readthedocs.io/en/stable/demos/basics/loading-pandas.html    \n",
    "\n",
    "https://stellargraph.readthedocs.io/en/v0.11.0/api.html\n",
    "\n",
    "\n",
    "Graph Regression Dataset\n",
    "https://paperswithcode.com/task/graph-regression/codeless\n",
    "\n",
    "StellerGraph Reference:\n",
    "https://stellargraph.readthedocs.io/en/stable/demos/time-series/gcn-lstm-time-series.html\n",
    "https://stellargraph.readthedocs.io\n",
    "\n",
    "GRaph CNN or similar\n",
    "It has multiple GCN layers and one 1d CNN + ... this idea might help\n",
    "https://stellargraph.readthedocs.io/en/stable/demos/graph-classification/dgcnn-graph-classification.html?highlight=cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References -- exploring ideas on the GCN-CNN\n",
    "https://ieeexplore.ieee.org/document/9149910\n",
    "\n",
    "https://antonsruberts.github.io/graph/gcn/\n",
    "\n",
    "This may work. As Unit GCN is created also unit tcn. This may give the opportunity to customize to product the correct output\n",
    "https://github.com/lshiwjx/2s-AGCN  https://paperswithcode.com/paper/non-local-graph-convolutional-networks-for\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from scracth and equations\n",
    "https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b\n",
    "\n",
    "https://jonathan-hui.medium.com/graph-convolutional-networks-gcn-pooling-839184205692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPC55bf7KmvCURxoGTa/FB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
